# 📦 批量导入题目使用指南

**创建时间**: 2025-10-05  
**适用对象**: 准备导入 1900 道题目

---

## 🎯 准备工作

### JSON 格式要求

**⚠️ 重要**：您的 JSON 文件必须符合标准格式。

#### ❌ 常见错误：

1. **中文引号**：
   ```json
   "content": "这是"错误的"格式"  ❌
   ```
   
2. **反引号**：
   ```json
   "text": "看到 `paradox`"  ❌
   ```

#### ✅ 正确格式：

1. **只使用英文双引号**：
   ```json
   "content": "这是正确的格式"  ✅
   ```

2. **不使用反引号**：
   ```json
   "text": "看到 paradox"  ✅
   ```

---

## 📋 批量导入流程

### 步骤 1: 准备 JSON 文件

1. **将所有题目 JSON 文件**放到 `content/unprocessed/` 目录
2. **确保文件名**以 `.json` 结尾
3. **建议命名**：`question_001.json`, `question_002.json` 等

---

### 步骤 2: 运行导入脚本

**在终端输入**：

```bash
npm run import:questions
```

**会发生什么**：
- 扫描 `content/unprocessed/` 目录
- 逐个导入 JSON 文件
- 自动处理词汇去重
- 显示详细进度

**示例输出**：
```
============================================================
🚀 Hajimi GRE 题目导入工具
============================================================

📂 找到 10 个JSON文件

[1/10] 处理文件: question_001.json
📝 开始导入题目: q_s1t1s1q1_victorians
  → 处理 8 个词汇...
  ✅ 创建新词汇: paradox
  ♻️  词汇已存在，复用: insular
  ...
✅ 题目导入成功

[2/10] 处理文件: question_002.json
...

============================================================
📊 导入完成统计
============================================================
  总文件数: 10
  ✅ 成功: 10
  ❌ 失败: 0
  📚 处理词汇: 65
============================================================
```

---

### 步骤 3: 处理导入错误

**如果有文件导入失败**：

1. **查看错误详情**：
   ```
   ❌ 失败详情:
     - question_005.json
       错误: Expected ',' or '}' after property value
   ```

2. **常见原因**：
   - JSON 格式错误（中文引号、反引号）
   - 缺少必需字段
   - 数据类型不匹配

3. **修复方法**：
   - 用文本编辑器打开文件
   - 查找并替换中文引号为英文引号
   - 删除或替换反引号
   - 重新导入

---

### 步骤 4: 在 Directus 中查看

1. **启动 Directus**（如果没启动）：
   ```bash
   cd cms
   .\start.bat
   ```

2. **访问**: http://localhost:8055

3. **登录**: admin@test.com / admin123456

4. **点击 Questions** 查看所有导入的题目

---

## 🔍 验证数据完整性

### 检查项目：

- [ ] 题目数量正确（questions 表）
- [ ] 选项数量正确（question_options 表）
- [ ] 词汇去重正常（vocabularies 表）
- [ ] 关联关系正确（question_vocabularies 表）

### 使用 Prisma Studio：

```bash
npx prisma studio
```

**查看**：
- Questions: 应该有 1900 条记录
- Vocabularies: 应该远少于 1900×8（因为去重）
- Question Options: 应该有 1900×5 = 9500 条（假设每题5个选项）

---

## 📊 批量导入建议

### 分批导入（推荐）

**为什么**：
- 便于管理
- 容易回滚
- 发现问题及时修复

**方案**：
```
第1批: 100 道题（测试）
第2批: 500 道题
第3批: 1300 道题
总计: 1900 道题
```

### 每批导入后：

1. **检查数据**：在 Directus 中查看
2. **验证去重**：检查词汇数量
3. **备份数据**：导出 SQL（可选）
4. **继续下一批**

---

## 🛠️ 故障排查

### 问题 1: 导入速度很慢

**原因**: 网络延迟或数据库连接不稳定

**解决**:
```bash
# 使用直接连接而非连接池
# 在 .env 中确认使用端口 5432
```

### 问题 2: 词汇没有去重

**检查**:
```sql
-- 在 Supabase SQL Editor 执行
SELECT word, COUNT(*) as count 
FROM vocabularies 
GROUP BY word 
HAVING COUNT(*) > 1;
```

**应该返回**: 空结果（没有重复）

### 问题 3: 内存不足

**症状**: 导入大量文件时崩溃

**解决**: 分批导入，每批 100-200 个文件

---

## ✅ 完成标志

导入完成后，您应该能：
- ✅ 在 Directus 中看到所有题目
- ✅ 搜索和筛选题目
- ✅ 查看词汇统计
- ✅ 编辑题目内容

---

**最后更新**: 2025-10-05  
**维护者**: Hajimi 开发团队

