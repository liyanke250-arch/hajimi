# 🤖 GRE题目自动化生成系统设计方案

**创建时间**: 2025-10-05  
**版本**: 1.0 - 设计阶段  
**状态**: 📝 设计中，未开始编程

---

## 🎯 系统目标

**核心目标**: 实现从原始题目到最终入库的全自动化流程

**输入**: 1900道GRE原始题目文本  
**输出**: 高质量的Markdown解析 + 标准JSON数据 + 自动入库

**关键要求**:
- 自动化程度 > 90%
- 质量保证机制
- 错误处理和重试
- 进度保存和恢复
- 人工审核节点

---

## 📊 系统架构设计

### 整体流程图

```
原始题目文本
    ↓
┌─────────────────────────────────────┐
│  阶段1: 生成 Markdown 解析          │
│  - 读取题目                          │
│  - 调用AI（使用V30 Prompt）         │
│  - 生成Markdown文件                 │
│  - 保存到 output/markdown/          │
└─────────────────────────────────────┘
    ↓
┌─────────────────────────────────────┐
│  阶段2: 质量审计                    │
│  - 读取Markdown                     │
│  - 调用审计AI（使用Audit Prompt）   │
│  - 检查是否合规                     │
└─────────────────────────────────────┘
    ↓
   合格？
    ├─ 是 → 继续
    └─ 否 → 记录问题 → 重新生成（最多3次）
    ↓
┌─────────────────────────────────────┐
│  阶段3: 生成 JSON                   │
│  - 从Markdown提取JSON部分           │
│  - 验证JSON格式                     │
│  - 保存到 output/json/              │
└─────────────────────────────────────┘
    ↓
┌─────────────────────────────────────┐
│  阶段4: 自动入库                    │
│  - 调用导入脚本                     │
│  - 词汇自动去重                     │
│  - 记录导入结果                     │
└─────────────────────────────────────┘
    ↓
完成 ✅
```

---

## 🔧 详细设计

### 阶段1: Markdown 生成

#### 输入
```
题目文本格式:
---
题目ID: Stage 1 Test 1 Section 1-8
题目文本: Scientists have long...
选项:
A. option1
B. option2
...
---
```

#### 处理流程
1. **读取题目文件**
   - 从 `input/questions/` 读取
   - 支持多种格式（txt, md, json）
   - 解析题目结构

2. **调用AI生成**
   - 使用 V30.0 Prompt
   - 传入题目信息
   - 设置超时时间（5分钟）
   - 重试机制（最多3次）

3. **保存Markdown**
   - 保存到 `output/markdown/stage1/test1/`
   - 文件名: `q_s1t1s1q8_[core_noun].md`
   - 同时保存元数据（生成时间、版本等）

#### 错误处理
- **AI不响应**: 等待30秒后重试，最多3次
- **生成内容不完整**: 提示并保存到 `output/errors/`
- **网络错误**: 记录错误，继续下一题

---

### 阶段2: 质量审计

#### 输入
- 刚生成的Markdown文件

#### 处理流程
1. **读取Markdown**
   - 从 `output/markdown/` 读取
   - 解析内容结构

2. **调用审计AI**
   - 使用 Audit V1.0 Prompt
   - 传入Markdown内容
   - 传入原始V30 Prompt作为规则
   - 获取审计结果JSON

3. **判断结果**
   ```json
   {
     "status": "pass" | "fail",
     "errors": [...]
   }
   ```

4. **分支处理**
   - **pass**: 进入下一阶段
   - **fail**: 
     - 记录错误详情
     - 尝试自动修复（简单问题）
     - 或标记为需要人工审核
     - 或重新生成（复杂问题）

#### 重试策略
```
第1次失败 → 尝试自动修复
第2次失败 → 重新生成（调整Prompt）
第3次失败 → 标记为待人工审核
```

---

### 阶段3: JSON 生成

#### 输入
- 通过审计的Markdown文件

#### 处理流程
1. **提取JSON部分**
   - 在Markdown中找到JSON代码块
   - 提取JSON字符串

2. **验证和清理**
   - 使用 Node.js JSON.parse() 验证
   - 检查必需字段
   - 验证数据类型

3. **保存JSON**
   - 保存到 `output/json/stage1/test1/`
   - 文件名与Markdown对应
   - 记录校验信息

#### 错误处理
- **JSON格式错误**: 记录错误位置，标记待人工修复
- **字段缺失**: 记录缺失字段，返回重新生成
- **数据类型错误**: 尝试自动转换或标记错误

---

### 阶段4: 自动入库

#### 输入
- 验证通过的JSON文件

#### 处理流程
1. **调用导入脚本**
   ```bash
   npm run import:questions output/json/stage1/test1
   ```

2. **监控导入结果**
   - 捕获成功/失败状态
   - 记录词汇去重统计
   - 保存导入日志

3. **更新状态**
   - 标记题目为"已入库"
   - 更新进度文件
   - 生成统计报告

---

## 🗂️ 文件组织结构

### 输入目录
```
input/
├── questions/           # 原始题目文本
│   ├── stage1/
│   │   ├── test1/
│   │   │   ├── q001.txt
│   │   │   └── ...
│   │   └── test2/
│   └── stage2/
└── prompts/
    ├── v30_generation.md  # 生成Prompt
    └── v1_audit.md        # 审计Prompt
```

### 输出目录
```
output/
├── markdown/            # 生成的Markdown
│   ├── stage1/
│   │   └── test1/
│   │       ├── q_s1t1s1q8_scientist.md
│   │       └── ...
├── json/                # 生成的JSON
│   ├── stage1/
│   │   └── test1/
│   │       ├── q_s1t1s1q8_scientist.json
│   │       └── ...
├── errors/              # 错误记录
│   ├── generation_errors.json
│   ├── audit_errors.json
│   └── import_errors.json
└── logs/                # 详细日志
    ├── 2025-10-05.log
    └── ...
```

### 状态目录
```
state/
├── progress.json        # 总体进度
├── current_batch.json   # 当前批次状态
└── checkpoints/         # 检查点（断点续传）
    └── stage1_test1.json
```

---

## 🔄 批处理策略

### 分批处理
```
为什么分批？
- 避免一次处理太多导致失败
- 便于检查和调试
- 可以随时暂停和恢复

批次大小建议:
- 小批量测试: 10道题
- 正常批量: 50道题
- 大批量: 100道题

总体规划:
- Stage 1 (28道题): 4个批次
- Stage 2 (28道题): 4个批次
- ...
- 总计: 约20-40个批次
```

### 断点续传
```json
// progress.json 示例
{
  "total_questions": 1900,
  "completed": 156,
  "failed": 3,
  "current_stage": "stage2",
  "current_test": "test1",
  "current_question": 8,
  "last_checkpoint": "2025-10-05T20:30:00Z"
}
```

---

## 🛡️ 质量保证机制

### 多层验证

#### 第1层: 生成时验证
```
检查项:
- AI是否返回了完整内容
- Markdown结构是否完整
- 必需模块是否都存在
```

#### 第2层: 审计验证
```
检查项:
- 结构完整性（所有模块）
- 词汇完整性（所有选项词汇）
- 格式规范性（标点、加粗）
- 内容纯净度（无污染字符）
```

#### 第3层: JSON验证
```
检查项:
- JSON格式正确性
- 必需字段完整性
- 数据类型正确性
- UUID唯一性
```

#### 第4层: 入库验证
```
检查项:
- 数据库插入成功
- 词汇去重正常
- 关联关系正确
```

### 质量分级

```
A级（完美）:
- 一次通过所有验证
- 自动入库
- 无需人工干预

B级（良好）:
- 经过1-2次修正后通过
- 自动入库
- 记录问题类型

C级（待审）:
- 多次生成仍有问题
- 保存到待审核目录
- 需要人工检查

D级（失败）:
- 严重错误无法修复
- 保存到失败目录
- 需要重新设计
```

---

## ⚠️ 错误处理策略

### 常见错误类型

#### 1. AI不响应
```
症状: 请求超时，无返回
原因: 网络问题、API限制、服务器故障
处理:
  1. 等待30秒
  2. 重试（最多3次）
  3. 跳过，记录到错误日志
  4. 稍后重新处理
```

#### 2. 生成内容不完整
```
症状: 缺少某些模块
原因: AI截断、Token限制
处理:
  1. 检查是否有关键模块
  2. 缺少次要模块 → 标记为B级
  3. 缺少关键模块 → 重新生成
  4. 多次仍不完整 → 待人工审核
```

#### 3. 审计不通过
```
症状: audit返回fail
原因: 格式问题、词汇遗漏、标点错误
处理:
  根据错误类型分类:
  - 简单错误（标点） → 自动修复
  - 中等错误（词汇遗漏） → 重新生成
  - 复杂错误（逻辑问题） → 待人工审核
```

#### 4. JSON解析失败
```
症状: JSON.parse()报错
原因: 格式错误、字符转义问题
处理:
  1. 尝试自动修复常见问题
  2. 无法修复 → 重新生成JSON部分
  3. 仍失败 → 待人工审核
```

#### 5. 导入失败
```
症状: 数据库插入失败
原因: 数据验证失败、重复UUID
处理:
  1. 检查错误类型
  2. UUID重复 → 生成新UUID
  3. 数据验证失败 → 记录详情
  4. 标记为待人工处理
```

---

## 🔁 重试机制设计

### 智能重试策略

```
重试决策树:

错误发生
    ↓
是否可自动修复？
├─ 是 → 自动修复 → 继续
└─ 否 → 是否值得重试？
         ├─ 是 → 重试次数 < 3？
         │        ├─ 是 → 重新生成
         │        └─ 否 → 标记待审核
         └─ 否 → 直接标记待审核
```

### 重试次数限制

| 错误类型 | 最大重试次数 | 重试间隔 |
|---------|-------------|---------|
| AI不响应 | 3次 | 30秒 |
| 内容不完整 | 2次 | 立即 |
| 审计不通过 | 2次 | 立即 |
| JSON格式错误 | 1次 | 立即 |
| 导入失败 | 1次 | 立即 |

---

## 💾 状态管理设计

### 进度追踪

```typescript
// progress.json 结构
{
  "project": {
    "name": "GRE 1900题",
    "start_time": "2025-10-06T09:00:00Z",
    "total_questions": 1900
  },
  "statistics": {
    "total": 1900,
    "completed": 156,
    "in_progress": 1,
    "failed": 3,
    "pending": 1740,
    "success_rate": "98.1%"
  },
  "current": {
    "stage": "stage2",
    "test": "test1",
    "question": 8,
    "status": "generating_markdown"
  },
  "quality": {
    "grade_a": 145,  // 完美
    "grade_b": 8,    // 良好
    "grade_c": 2,    // 待审
    "grade_d": 1     // 失败
  },
  "last_updated": "2025-10-06T15:30:00Z"
}
```

### 检查点机制

```typescript
// checkpoint.json 结构
{
  "checkpoint_id": "stage2_test1_batch2",
  "timestamp": "2025-10-06T15:30:00Z",
  "completed_questions": [
    "q_s2t1s1q1_rainfall",
    "q_s2t1s1q2_recovery",
    // ...
  ],
  "failed_questions": [
    {
      "id": "q_s2t1s1q5_taste",
      "error_type": "audit_failed",
      "retry_count": 2
    }
  ],
  "next_question": "q_s2t1s1q6_maze"
}
```

---

## 🚨 异常情况处理

### 1. AI长时间不响应

**检测**: 超过5分钟无响应

**处理方案**:
```
1. 自动保存当前进度
2. 记录错误信息
3. 暂停当前批次
4. 发送通知（如果配置了）
5. 等待用户决定:
   - 继续等待
   - 跳过当前题目
   - 暂停整个流程
```

### 2. 连续多题失败

**检测**: 连续5道题都失败

**处理方案**:
```
1. 立即暂停流程
2. 分析失败原因
3. 记录详细日志
4. 提示用户检查:
   - Prompt是否有问题
   - 网络是否正常
   - AI是否正常
5. 等待用户确认后继续
```

### 3. 磁盘空间不足

**检测**: 保存文件时失败

**处理方案**:
```
1. 立即停止生成
2. 保存当前进度
3. 清理临时文件
4. 提示用户清理磁盘
5. 等待确认后继续
```

### 4. 数据库连接失败

**检测**: 导入时无法连接

**处理方案**:
```
1. 停止导入
2. JSON文件已生成，不会丢失
3. 记录导入队列
4. 提示用户检查数据库
5. 连接恢复后继续导入
```

---

## 📈 进度监控设计

### 实时监控界面（可选）

```
┌─────────────────────────────────────┐
│  Hajimi 自动化生成系统              │
│  ─────────────────────────────────  │
│  总进度: ████████░░ 156/1900 (8%)  │
│  当前: Stage 2 Test 1 Question 8    │
│  状态: 正在生成Markdown...          │
│  ─────────────────────────────────  │
│  统计:                              │
│  ✅ 成功: 145 (A级)                │
│  🔄 良好: 8   (B级)                │
│  ⚠️  待审: 2   (C级)                │
│  ❌ 失败: 1   (D级)                │
│  ─────────────────────────────────  │
│  最近10题:                          │
│  q8: ✅ | q7: ✅ | q6: ✅ | ...     │
└─────────────────────────────────────┘
```

### 日志系统

```
日志级别:
- INFO: 正常流程（开始、完成）
- WARN: 警告（重试、自动修复）
- ERROR: 错误（失败、异常）
- DEBUG: 调试信息（详细过程）

日志格式:
[2025-10-06 15:30:45] [INFO] [Stage2-Test1-Q8] 开始生成Markdown
[2025-10-06 15:31:20] [INFO] [Stage2-Test1-Q8] Markdown生成成功
[2025-10-06 15:31:25] [INFO] [Stage2-Test1-Q8] 开始审计
[2025-10-06 15:31:30] [WARN] [Stage2-Test1-Q8] 审计发现2处问题，尝试修复
[2025-10-06 15:31:35] [INFO] [Stage2-Test1-Q8] 修复成功，审计通过
[2025-10-06 15:31:40] [INFO] [Stage2-Test1-Q8] JSON生成成功
[2025-10-06 15:31:50] [INFO] [Stage2-Test1-Q8] 导入成功
```

---

## 🎛️ 控制参数设计

### 可配置参数

```typescript
// config.json
{
  "generation": {
    "batch_size": 50,           // 每批处理多少题
    "ai_timeout": 300000,       // AI响应超时（5分钟）
    "max_retries": 3,           // 最大重试次数
    "retry_delay": 30000        // 重试延迟（30秒）
  },
  "audit": {
    "enabled": true,            // 是否启用审计
    "strict_mode": false,       // 严格模式（小错误也拒绝）
    "auto_fix": true            // 自动修复简单问题
  },
  "import": {
    "auto_import": true,        // 自动导入
    "verify_after_import": true // 导入后验证
  },
  "quality": {
    "min_grade": "B",           // 最低接受等级
    "manual_review_grade": "C"  // 需要人工审核的等级
  },
  "safety": {
    "pause_on_consecutive_fails": 5,  // 连续失败多少题后暂停
    "checkpoint_interval": 10          // 每10题保存一次检查点
  }
}
```

---

## 🚦 人工介入节点

### 必需的人工介入

1. **开始前**: 
   - 确认Prompt配置
   - 确认题目文件准备好
   - 确认数据库连接正常

2. **批次之间**:
   - 检查上一批次的质量
   - 决定是否继续
   - 调整参数（如果需要）

3. **遇到连续失败**:
   - 分析失败原因
   - 决定是否继续
   - 调整策略

### 可选的人工介入

1. **质量抽查**:
   - 每50题抽查5题
   - 检查质量是否符合预期

2. **C级题目审核**:
   - 查看待审核题目
   - 手动修复或重新生成

---

## 📊 输出报告设计

### 每日报告
```markdown
# 自动化生成日报 - 2025-10-06

## 今日统计
- 处理题目: 150道
- 成功率: 97.3%
- A级: 138道 (92%)
- B级: 8道 (5.3%)
- C级: 2道 (1.3%)
- D级: 2道 (1.3%)

## 效率统计
- 平均每题耗时: 45秒
- 重试次数: 12次
- 自动修复成功: 8次

## 问题汇总
- 最常见错误: 词汇遗漏（5次）
- 需要人工审核: 2道题

## 明日计划
- 继续Stage 3
- 预计完成: 150道
```

---

## 🎯 实施阶段规划

### Phase 1: 原型验证（1天）
```
目标: 验证整个流程可行性
范围: 10道题
输出: 
  - 工作的原型脚本
  - 验证所有环节
  - 发现潜在问题
```

### Phase 2: 小规模测试（2-3天）
```
目标: 优化和完善
范围: 50-100道题
输出:
  - 稳定的流程
  - 完善的错误处理
  - 质量保证机制
```

### Phase 3: 批量生产（1-2周）
```
目标: 完成所有题目
范围: 1900道题
输出:
  - 完整的题库
  - 质量报告
  - 统计分析
```

---

## 🤔 关键决策点

### 需要您决定的问题

#### 1. AI调用方式
```
选项A: 手动调用（推荐开始时）
- 您在Cursor中运行
- 逐题生成和审计
- 完全可控

选项B: 半自动（推荐过渡期）
- 脚本批量调用AI
- 您监控和干预
- 平衡效率和控制

选项C: 全自动（推荐成熟后）
- 完全自动化
- 只看报告
- 最高效率
```

#### 2. 质量vs效率
```
严格模式（质量优先）:
- 所有题目必须A级
- 发现问题立即暂停
- 生成速度较慢

平衡模式（推荐）:
- 接受A级和B级
- C级待审核
- 效率和质量平衡

快速模式（效率优先）:
- 接受所有非D级
- 人工最后统一审核
- 最快完成
```

#### 3. 审核时机
```
实时审核（推荐）:
- 每道题生成后立即审计
- 及时发现问题
- 便于调整

批量审核:
- 生成一批后统一审计
- 效率更高
- 但问题发现较晚
```

---

## 💡 技术实现建议

### 推荐技术栈

```
脚本语言: TypeScript
- 类型安全
- 与项目统一
- 便于维护

AI交互: 
- 方案A: 手动在Cursor中调用
- 方案B: OpenAI API（需要密钥）
- 方案C: 其他AI API

文件操作: Node.js fs模块
进度显示: cli-progress（已安装）
日志: winston 或 pino
配置: JSON文件
```

### 不推荐的方案

❌ **Python脚本**: 与项目技术栈不统一  
❌ **完全自动无监控**: 风险太高  
❌ **无断点续传**: 失败后要重头开始  
❌ **无质量检查**: 垃圾进，垃圾出  

---

## 📝 下一步行动

### 立即可做（设计阶段）

1. **确认需求**
   - 您想要什么程度的自动化？
   - 质量要求是什么？
   - 可接受的失败率？

2. **准备资源**
   - 题目文本是否准备好？
   - Prompt文件位置确认？
   - 输出目录规划？

3. **原型设计**
   - 用10道题测试流程
   - 验证每个环节
   - 调整设计

### 开发阶段（设计确认后）

1. **Phase 1: 基础脚本**（1天）
   - 题目读取
   - AI调用封装
   - 文件保存

2. **Phase 2: 审计集成**（1天）
   - 审计流程
   - 错误处理
   - 自动修复

3. **Phase 3: 完善系统**（2-3天）
   - 进度追踪
   - 日志系统
   - 监控界面

---

## 🎯 成功标准

### 系统验收标准

- [ ] 能处理1900道题
- [ ] 成功率 > 95%
- [ ] A级+B级 > 90%
- [ ] 平均每题耗时 < 2分钟
- [ ] 支持断点续传
- [ ] 完整的错误日志
- [ ] 质量统计报告

---

**最后更新**: 2025-10-05  
**状态**: 设计完成，等待确认  
**下一步**: 确认需求后开始原型开发
